{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction For Input:  [1.0213361012275326, 1.8117747114069398, 3.3926519317657537, 2.6022133215863468, 4.183090541945161]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = [1, 2, 4, 3, 5]\n",
    "y = [1, 3, 3, 2, 5]\n",
    "\n",
    "epoch = 4\n",
    "learning_rate = 0.01\n",
    "b0 = 0.0\n",
    "b1 = 0.0\n",
    "\n",
    "bias = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    \n",
    "    for j in range(len(x)):\n",
    "        # Calculate predition for p(j) = b0 + b1 * x(j)\n",
    "        p = b0 + b1 * x[j]\n",
    "        \n",
    "        # Calculate Error, where error = p(j) - y(j)\n",
    "        error = p - y[j]\n",
    "        \n",
    "        # Calculate the gradient descent to update the weight\n",
    "        b0 = b0 - learning_rate * error\n",
    "        b1 = b1 - learning_rate * error * x[j]\n",
    "        \n",
    "        bias.append([b0, b1])\n",
    "\n",
    "\n",
    "predictionX = []\n",
    "\n",
    "for i in range(len(x)):\n",
    "    main_bias = bias[- 1]\n",
    "    y = main_bias[0] + main_bias[1] * x[i]\n",
    "    predictionX.append(y)\n",
    "\n",
    "print(\"Prediction For Input: \", predictionX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Stoichastic Gradient Desent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [2.7810836, 2.550537003],\n",
    "    [1.465489372, 2.362125076],\n",
    "    [3.396561688, 4.400293529],\n",
    "    [1.38807019, 1.850220317],\n",
    "    [3.06407232, 3.005305973],\n",
    "    [7.627531214, 2.759262235],\n",
    "    [5.332441248, 2.088626775],\n",
    "    [6.922596716, 1.77106367],\n",
    "    [8.675418651, -0.242068655],\n",
    "    [7.673756466, 3.508563011],\n",
    "])\n",
    "Y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
    "learning_rate = 0.3\n",
    "weight = np.zeros(X.shape[1])\n",
    "storedWeight = []\n",
    "bias = 0.0\n",
    "storedBias = []\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight:  [-0.10429064 -0.09564514]  Bias:  -0.0375\n",
      "Weight:  [-0.14739695 -0.16512535]  Bias:  -0.0669142796436\n",
      "Weight:  [-0.18788205 -0.21757437]  Bias:  -0.0788337171592\n",
      "Weight:  [-0.21965145 -0.25992121]  Bias:  -0.10172117627\n",
      "Weight:  [-0.24637087 -0.28612817]  Bias:  -0.110441405083\n",
      "Weight:  [-0.11660757 -0.23918625]  Bias:  -0.0934289152407\n",
      "Weight:  [ 0.10704584 -0.151585  ]  Bias:  -0.0514868850365\n",
      "Weight:  [ 0.29569144 -0.10332228]  Bias:  -0.0242361869311\n",
      "Weight:  [ 0.30747607 -0.1036511 ]  Bias:  -0.0228777938985\n",
      "Weight:  [ 0.33648636 -0.09038714]  Bias:  -0.0190973386667\n",
      "Weight:  [ 0.21288868 -0.2037388 ]  Bias:  -0.0635396121202\n",
      "Weight:  [ 0.16293166 -0.28426121]  Bias:  -0.0976285811453\n",
      "Weight:  [ 0.08777519 -0.38162747]  Bias:  -0.119755801659\n",
      "Weight:  [ 0.05351792 -0.42729051]  Bias:  -0.144435583147\n",
      "Weight:  [ 0.01157973 -0.46842436]  Bias:  -0.158122659531\n",
      "Weight:  [ 0.32402117 -0.35539855]  Bias:  -0.117160330741\n",
      "Weight:  [ 0.41007829 -0.32169144]  Bias:  -0.101021922141\n",
      "Weight:  [ 0.42660901 -0.31746225]  Bias:  -0.0986339848131\n",
      "Weight:  [ 0.42788117 -0.31749775]  Bias:  -0.0984873453442\n",
      "Weight:  [ 0.44959813 -0.30756841]  Bias:  -0.0956573151226\n",
      "Weight:  [ 0.32815337 -0.41894563]  Bias:  -0.139325462144\n",
      "Weight:  [ 0.28944055 -0.48134426]  Bias:  -0.165741770032\n",
      "Weight:  [ 0.24381445 -0.54045351]  Bias:  -0.179174794453\n",
      "Weight:  [ 0.21198814 -0.58287621]  Bias:  -0.202103256742\n",
      "Weight:  [ 0.1690912  -0.62495042]  Bias:  -0.216103231075\n",
      "Weight:  [ 0.50042832 -0.5050891 ]  Bias:  -0.172663603475\n",
      "Weight:  [ 0.53723066 -0.49067424]  Bias:  -0.16576201008\n",
      "Weight:  [ 0.5431006  -0.48917249]  Bias:  -0.164914070761\n",
      "Weight:  [ 0.54326268 -0.48917701]  Bias:  -0.164895388071\n",
      "Weight:  [ 0.55657605 -0.48308993]  Bias:  -0.163160466344\n",
      "Weight:  [ 0.43891324 -0.59099871]  Bias:  -0.205468725742\n",
      "Weight:  [ 0.4082685  -0.64039294]  Bias:  -0.226379655977\n",
      "Weight:  [ 0.37766335 -0.68004234]  Bias:  -0.235390281683\n",
      "Weight:  [ 0.34806864 -0.71949044]  Bias:  -0.256711039714\n",
      "Weight:  [ 0.30482748 -0.76190227]  Bias:  -0.270823357278\n",
      "Weight:  [ 0.55599831 -0.67104113]  Bias:  -0.237893852235\n",
      "Weight:  [ 0.59566646 -0.65550379]  Bias:  -0.230454829037\n",
      "Weight:  [ 0.60043556 -0.65428367]  Bias:  -0.229765911158\n",
      "Weight:  [ 0.60049144 -0.65428523]  Bias:  -0.229759469739\n",
      "Weight:  [ 0.617496   -0.64651048]  Bias:  -0.227543533355\n",
      "Weight:  [ 0.50976805 -0.74530799]  Bias:  -0.266279497004\n",
      "Weight:  [ 0.48689215 -0.78218013]  Bias:  -0.281889231392\n",
      "Weight:  [ 0.46896383 -0.80540652]  Bias:  -0.287167603734\n",
      "Weight:  [ 0.44249584 -0.84068688]  Bias:  -0.306235798327\n",
      "Weight:  [ 0.40317612 -0.87925248]  Bias:  -0.31906830148\n",
      "Weight:  [ 0.580186   -0.81521909]  Bias:  -0.295861594473\n",
      "Weight:  [ 0.63103252 -0.79530337]  Bias:  -0.28632627702\n",
      "Weight:  [ 0.63582615 -0.79407698]  Bias:  -0.285633816466\n",
      "Weight:  [ 0.63585452 -0.79407777]  Bias:  -0.285630545646\n",
      "Weight:  [ 0.66053953 -0.7827914 ]  Bias:  -0.282413737386\n",
      "Weight:  [ 0.56515776 -0.8702662 ]  Bias:  -0.31671035807\n",
      "Weight:  [ 0.54769461 -0.89841389]  Bias:  -0.328626616102\n",
      "Weight:  [ 0.53676278 -0.91257623]  Bias:  -0.331845115404\n",
      "Weight:  [ 0.51322776 -0.9439471 ]  Bias:  -0.348800324087\n",
      "Weight:  [ 0.47846092 -0.97804714]  Bias:  -0.360146933991\n",
      "Weight:  [ 0.60668273 -0.93166285]  Bias:  -0.343336539739\n",
      "Weight:  [ 0.66519242 -0.9087456 ]  Bias:  -0.332364137867\n",
      "Weight:  [ 0.66968008 -0.90759748]  Bias:  -0.331715874848\n",
      "Weight:  [ 0.66969505 -0.9075979 ]  Bias:  -0.331714149526\n",
      "Weight:  [ 0.70072206 -0.89341186]  Bias:  -0.327670887378\n",
      "Weight:  [ 0.615812   -0.97128304]  Bias:  -0.358202174051\n",
      "Weight:  [ 0.60185193 -0.99378433]  Bias:  -0.367728045045\n",
      "Weight:  [ 0.59453474 -1.00326386]  Bias:  -0.369882339506\n",
      "Weight:  [ 0.57332125 -1.03154027]  Bias:  -0.385165062761\n",
      "Weight:  [ 0.54209935 -1.06216336]  Bias:  -0.395354736717\n",
      "Weight:  [ 0.63582312 -1.02825875]  Bias:  -0.383067174264\n",
      "Weight:  [ 0.69808358 -1.00387238]  Bias:  -0.371391384729\n",
      "Weight:  [ 0.70210295 -1.00284407]  Bias:  -0.370810767974\n",
      "Weight:  [ 0.70211111 -1.0028443 ]  Bias:  -0.370809827501\n",
      "Weight:  [ 0.73772389 -0.98656157]  Bias:  -0.366168973795\n",
      "Weight:  [ 0.66142467 -1.05653574]  Bias:  -0.39360404441\n",
      "Weight:  [ 0.64991387 -1.07508922]  Bias:  -0.401458619085\n",
      "Weight:  [ 0.64471398 -1.08182575]  Bias:  -0.402989546215\n",
      "Weight:  [ 0.62539113 -1.10758204]  Bias:  -0.416910205003\n",
      "Weight:  [ 0.59703547 -1.13539386]  Bias:  -0.426164444776\n",
      "Weight:  [ 0.66676133 -1.11017051]  Bias:  -0.417023105047\n",
      "Weight:  [ 0.72952823 -1.08558578]  Bias:  -0.405252342362\n",
      "Weight:  [ 0.73303511 -1.08468858]  Bias:  -0.404745758171\n",
      "Weight:  [ 0.7330397  -1.08468871]  Bias:  -0.404745228958\n",
      "Weight:  [ 0.77158882 -1.06706344]  Bias:  -0.39972172758\n",
      "Weight:  [ 0.70240997 -1.1305075 ]  Bias:  -0.424596512046\n",
      "Weight:  [ 0.69270592 -1.14614881]  Bias:  -0.431218222935\n",
      "Weight:  [ 0.68885435 -1.15113857]  Bias:  -0.432352184524\n",
      "Weight:  [ 0.67111076 -1.17478979]  Bias:  -0.445135102716\n",
      "Weight:  [ 0.64515183 -1.20025085]  Bias:  -0.453607140533\n",
      "Weight:  [ 0.69814694 -1.18107985]  Bias:  -0.446659267618\n",
      "Weight:  [ 0.75932653 -1.15711684]  Bias:  -0.435186176157\n",
      "Weight:  [ 0.76234632 -1.15634427]  Bias:  -0.434749954094\n",
      "Weight:  [ 0.76234899 -1.15634434]  Bias:  -0.43474964641\n",
      "Weight:  [ 0.80254267 -1.13796715]  Bias:  -0.429511836474\n",
      "Weight:  [ 0.73933536 -1.1959347 ]  Bias:  -0.452239420112\n",
      "Weight:  [ 0.73101609 -1.20934397]  Bias:  -0.457916201543\n",
      "Weight:  [ 0.72807465 -1.21315465]  Bias:  -0.458782207334\n",
      "Weight:  [ 0.71167642 -1.23501258]  Bias:  -0.470595900181\n",
      "Weight:  [ 0.68777126 -1.25845926]  Bias:  -0.478397660331\n",
      "Weight:  [ 0.72895676 -1.24356039]  Bias:  -0.472998076178\n",
      "Weight:  [ 0.78742046 -1.22066115]  Bias:  -0.46203429869\n",
      "Weight:  [ 0.79000745 -1.2199993 ]  Bias:  -0.461660595401\n",
      "Weight:  [ 0.79000906 -1.21999934]  Bias:  -0.461660410652\n",
      "Weight:  [ 0.83090803 -1.20129968]  Bias:  -0.456330690412\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(weight, input):\n",
    "    sum = 0\n",
    "    for w, inp in zip(weight, input):\n",
    "        sum += (w * inp)\n",
    "    sum = 1/(1 + np.exp(-sum))\n",
    "    return sum\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    for x, y in zip(X, Y):\n",
    "        \n",
    "        # Calculate the prediction assuming bias = 0.0 and weight = 0.0\n",
    "        prediction = sigmoid(weight, x)\n",
    "        \n",
    "        # Calculate the co-efficients using this formula\n",
    "        # b = bias + learning_rate * (y - prediction) * prediction * (1 - prediction) * input\n",
    "        bias = bias + learning_rate * (y - prediction) * prediction * (1 - prediction) * 1.0\n",
    "        storedBias.append(bias)\n",
    "        for i in range(len(x)):\n",
    "            weight[i] = weight[i] + learning_rate * (y - prediction) * prediction * (1 - prediction) * x[i]\n",
    "        storedWeight.append(weight)\n",
    "        print(\"Weight: \", weight, \" Bias: \", bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:  2.7810836\n",
      "2:  2.550537003\n",
      "Predicted Sum:  -0.85322400958\n"
     ]
    }
   ],
   "source": [
    "# b0 = -0.456330690412\n",
    "# b1 = 0.83090803\n",
    "# b2 = -1.20129968\n",
    "b0 = -0.406605464\n",
    "b1 = 0.852573316\n",
    "b2 = -1.104746259\n",
    "\n",
    "print(\"1: \", X[0][0])\n",
    "print(\"2: \", X[0][1])\n",
    "sum = b0 + (X[0][0] * b1) + (X[0][1] * b2)\n",
    "print(\"Predicted Sum: \", sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
